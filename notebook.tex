
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dlnd\_tv\_script\_generation}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{tv-script-generation}{%
\section{TV Script Generation}\label{tv-script-generation}}

In this project, you'll generate your own
\href{https://en.wikipedia.org/wiki/The_Simpsons}{Simpsons} TV scripts
using RNNs. You'll be using part of the
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{Simpsons
dataset} of scripts from 27 seasons. The Neural Network you'll build
will generate a new TV script for a scene at
\href{https://simpsonswiki.com/wiki/Moe's_Tavern}{Moe's Tavern}. \#\#
Get the Data The data is already provided for you. You'll be using a
subset of the original dataset. It consists of only the scenes in Moe's
Tavern. This doesn't include other versions of the tavern, like ``Moe's
Cavern'', ``Flaming Moe's'', ``Uncle Moe's Family Feed-Bag'', etc..

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        
        \PY{n}{data\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/simpsons/moes\PYZus{}tavern\PYZus{}lines.txt}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{text} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Ignore notice, since we don\PYZsq{}t use it for analysing the data}
        \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{p}{[}\PY{l+m+mi}{81}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \hypertarget{explore-the-data}{%
\subsection{Explore the Data}\label{explore-the-data}}

Play around with \texttt{view\_sentence\_range} to view different parts
of the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{view\PYZus{}sentence\PYZus{}range} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset Stats}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Roughly the number of unique words: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{word}\PY{p}{:} \PY{k+kc}{None} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{scenes} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of scenes: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scenes}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{sentence\PYZus{}count\PYZus{}scene} \PY{o}{=} \PY{p}{[}\PY{n}{scene}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of sentences in each scene: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{sentence\PYZus{}count\PYZus{}scene}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{sentences} \PY{o}{=} \PY{p}{[}\PY{n}{sentence} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{scene}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of lines: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentences}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{word\PYZus{}count\PYZus{}sentence} \PY{o}{=} \PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{sentences}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of words in each line: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{word\PYZus{}count\PYZus{}sentence}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The sentences }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ to }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{o}{*}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Dataset Stats
Roughly the number of unique words: 11492
Number of scenes: 262
Average number of sentences in each scene: 15.248091603053435
Number of lines: 4257
Average number of words in each line: 11.50434578341555

The sentences 0 to 10:
Moe\_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.
Bart\_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.
Moe\_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?
Moe\_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.
Moe\_Szyslak: What's the matter Homer? You're not your normal effervescent self.
Homer\_Simpson: I got my problems, Moe. Give me another one.
Moe\_Szyslak: Homer, hey, you should not drink to forget your problems.
Barney\_Gumble: Yeah, you should only drink to enhance your social skills.



    \end{Verbatim}

    \hypertarget{implement-preprocessing-functions}{%
\subsection{Implement Preprocessing
Functions}\label{implement-preprocessing-functions}}

The first thing to do to any dataset is preprocessing. Implement the
following preprocessing functions below: - Lookup Table - Tokenize
Punctuation

\hypertarget{lookup-table}{%
\subsubsection{Lookup Table}\label{lookup-table}}

To create a word embedding, you first need to transform the words to
ids. In this function, create two dictionaries: - Dictionary to go from
the words to an id, we'll call \texttt{vocab\_to\_int} - Dictionary to
go from the id to word, we'll call \texttt{int\_to\_vocab}

Return these dictionaries in the following tuple
\texttt{(vocab\_to\_int,\ int\_to\_vocab)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{Counter}
        
        \PY{k}{def} \PY{n+nf}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create lookup tables for vocabulary}
        \PY{l+s+sd}{    :param text: The text of tv scripts split into words}
        \PY{l+s+sd}{    :return: A tuple of dicts (vocab\PYZus{}to\PYZus{}int, int\PYZus{}to\PYZus{}vocab)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{word\PYZus{}counts} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{text}\PY{p}{)}
            \PY{n}{sorted\PYZus{}vocab} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{word\PYZus{}counts}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{word\PYZus{}counts}\PY{o}{.}\PY{n}{get}\PY{p}{,} \PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
            \PY{n}{int\PYZus{}to\PYZus{}vocab} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{ii}\PY{p}{:} \PY{n}{word} \PY{k}{for} \PY{n}{ii}\PY{p}{,} \PY{n}{word} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sorted\PYZus{}vocab}\PY{p}{)}\PY{p}{\PYZcb{}}
            \PY{n}{vocab\PYZus{}to\PYZus{}int} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{word}\PY{p}{:} \PY{n}{ii} \PY{k}{for} \PY{n}{ii}\PY{p}{,} \PY{n}{word} \PY{o+ow}{in} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
            \PY{k}{return} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{tokenize-punctuation}{%
\subsubsection{Tokenize Punctuation}\label{tokenize-punctuation}}

We'll be splitting the script into a word array using spaces as
delimiters. However, punctuations like periods and exclamation marks
make it hard for the neural network to distinguish between the word
``bye'' and ``bye!''.

Implement the function \texttt{token\_lookup} to return a dict that will
be used to tokenize symbols like ``!'' into
``\textbar{}\textbar{}Exclamation\_Mark\textbar{}\textbar{}''. Create a
dictionary for the following symbols where the symbol is the key and
value is the token: - Period ( . ) - Comma ( , ) - Quotation Mark ( " )
- Semicolon ( ; ) - Exclamation mark ( ! ) - Question mark ( ? ) - Left
Parentheses ( ( ) - Right Parentheses ( ) ) - Dash ( -- ) - Return (
\n )

This dictionary will be used to token the symbols and add the delimiter
(space) around it. This separates the symbols as it's own word, making
it easier for the neural network to predict on the next word. Make sure
you don't use a token that could be confused as a word. Instead of using
the token ``dash'', try using something like
``\textbar{}\textbar{}dash\textbar{}\textbar{}''.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{token\PYZus{}lookup}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Generate a dict to turn punctuation into a token.}
        \PY{l+s+sd}{    :return: Tokenize dictionary where the key is the punctuation and the value is the token}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{tokenize\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||period||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||comma||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||quotation\PYZus{}mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{;}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||semicolon||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||exclamation\PYZus{}mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||question\PYZus{}mark||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||left\PYZus{}parentheses||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||right\PYZus{}parentheses||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||dash||}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{||return||}\PY{l+s+s1}{\PYZsq{}}
            \PY{p}{\PYZcb{}}
            
            \PY{k}{return} \PY{n}{tokenize\PYZus{}dict}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}tokenize}\PY{p}{(}\PY{n}{token\PYZus{}lookup}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{preprocess-all-the-data-and-save-it}{%
\subsection{Preprocess all the data and save
it}\label{preprocess-all-the-data-and-save-it}}

Running the code cell below will preprocess all the data and save it to
file.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Preprocess Training, Validation, and Testing Data}
        \PY{n}{helper}\PY{o}{.}\PY{n}{preprocess\PYZus{}and\PYZus{}save\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{,} \PY{n}{token\PYZus{}lookup}\PY{p}{,} \PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \hypertarget{check-point}{%
\section{Check Point}\label{check-point}}

This is your first checkpoint. If you ever decide to come back to this
notebook or have to restart the notebook, you can start from here. The
preprocessed data has been saved to disk.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        
        \PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \hypertarget{build-the-neural-network}{%
\subsection{Build the Neural Network}\label{build-the-neural-network}}

You'll build the components necessary to build a RNN by implementing the
following functions below: - get\_inputs - get\_init\_cell - get\_embed
- build\_rnn - build\_nn - get\_batches

\hypertarget{check-the-version-of-tensorflow-and-access-to-gpu}{%
\subsubsection{Check the Version of TensorFlow and Access to
GPU}\label{check-the-version-of-tensorflow-and-access-to-gpu}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{from} \PY{n+nn}{distutils}\PY{n+nn}{.}\PY{n+nn}{version} \PY{k}{import} \PY{n}{LooseVersion}
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        
        \PY{c+c1}{\PYZsh{} Check TensorFlow Version}
        \PY{k}{assert} \PY{n}{LooseVersion}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{LooseVersion}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1.3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Please use TensorFlow version 1.3 or newer}\PY{l+s+s1}{\PYZsq{}}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow Version: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Check for a GPU}
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{warnings}\PY{o}{.}\PY{n}{warn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No GPU found. Please use a GPU to train your neural network.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Default GPU Device: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TensorFlow Version: 1.3.0
Default GPU Device: /gpu:0

    \end{Verbatim}

    \hypertarget{input}{%
\subsubsection{Input}\label{input}}

Implement the \texttt{get\_inputs()} function to create TF Placeholders
for the Neural Network. It should create the following placeholders: -
Input text placeholder named ``input'' using the
\href{https://www.tensorflow.org/api_docs/python/tf/placeholder}{TF
Placeholder} \texttt{name} parameter. - Targets placeholder - Learning
Rate placeholder

Return the placeholders in the following tuple
\texttt{(Input,\ Targets,\ LearningRate)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create TF Placeholders for input, targets, and learning rate.}
        \PY{l+s+sd}{    :return: Tuple (input, targets, learning rate)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{inputs} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{int32}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{)}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{input}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{targets} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{int32}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{)}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{targets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{placeholder}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{inputs}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{learning\PYZus{}rate}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}inputs}\PY{p}{(}\PY{n}{get\PYZus{}inputs}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{build-rnn-cell-and-initialize}{%
\subsubsection{Build RNN Cell and
Initialize}\label{build-rnn-cell-and-initialize}}

Stack one or more
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell}{\texttt{BasicLSTMCells}}
in a
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell}{\texttt{MultiRNNCell}}.
- The Rnn size should be set using \texttt{rnn\_size} - Initalize Cell
State using the MultiRNNCell's
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell\#zero_state}{\texttt{zero\_state()}}
function - Apply the name ``initial\_state'' to the initial state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the cell and initial state in the following tuple
\texttt{(Cell,\ InitialState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{add\PYZus{}dropout}\PY{p}{(}\PY{n}{lstm\PYZus{}cell}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Add dropout to the cell.}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{        Arguments}
        \PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
        \PY{l+s+sd}{        lstm\PYZus{}cell: tensorflow BasicLSTMCell() object}
        \PY{l+s+sd}{        keep\PYZus{}prob: Scalar tensor (tf.placeholder) for the }
        \PY{l+s+sd}{                   dropout keep probability}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{k}{return} \PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{rnn}\PY{o}{.}\PY{n}{DropoutWrapper}\PY{p}{(}\PY{n}{lstm\PYZus{}cell}\PY{p}{,} \PY{n}{output\PYZus{}keep\PYZus{}prob}\PY{o}{=}\PY{n}{keep\PYZus{}prob}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create an RNN Cell and initialize it.}
        \PY{l+s+sd}{    :param batch\PYZus{}size: Size of batches}
        \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of RNNs}
        \PY{l+s+sd}{    :return: Tuple (cell, initialize state)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            
            \PY{c+c1}{\PYZsh{} Probability an LSTM cell will not be dropped.}
            \PY{n}{keep\PYZus{}prob} \PY{o}{=} \PY{l+m+mf}{0.5}
            \PY{c+c1}{\PYZsh{} Create an LSTM cell}
            \PY{n}{cell} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{rnn}\PY{o}{.}\PY{n}{BasicLSTMCell}\PY{p}{(}\PY{n}{rnn\PYZus{}size}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Stack multiple LSTM cells according to the number }
            \PY{c+c1}{\PYZsh{} of layers (rnn\PYZus{}layers). Wrap each LSTM cell in a }
            \PY{c+c1}{\PYZsh{} dropout layer.}
            \PY{n}{cell} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{rnn\PYZus{}cell}\PY{o}{.}\PY{n}{MultiRNNCell}\PY{p}{(}\PY{p}{[}\PY{n}{add\PYZus{}dropout}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{keep\PYZus{}prob}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}Create initial state.}
            \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{n}{cell}\PY{o}{.}\PY{n}{zero\PYZus{}state}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{initial\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{cell}\PY{p}{,} \PY{n}{initial\PYZus{}state}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{word-embedding}{%
\subsubsection{Word Embedding}\label{word-embedding}}

Apply embedding to \texttt{input\_data} using TensorFlow. Return the
embedded sequence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}embed}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create embedding for \PYZlt{}input\PYZus{}data\PYZgt{}.}
        \PY{l+s+sd}{    :param input\PYZus{}data: TF placeholder for text input.}
        \PY{l+s+sd}{    :param vocab\PYZus{}size: Number of words in vocabulary.}
        \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
        \PY{l+s+sd}{    :return: Embedded input.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{embedding} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Variable}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{random\PYZus{}uniform}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{,} 
                                                      \PY{n}{minval}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{maxval}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{embed} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{embedding\PYZus{}lookup}\PY{p}{(}\PY{n}{params}\PY{o}{=}\PY{n}{embedding}\PY{p}{,} \PY{n}{ids}\PY{o}{=}\PY{n}{input\PYZus{}data}\PY{p}{)}
            \PY{k}{return} \PY{n}{embed}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}embed}\PY{p}{(}\PY{n}{get\PYZus{}embed}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{build-rnn}{%
\subsubsection{Build RNN}\label{build-rnn}}

You created a RNN Cell in the \texttt{get\_init\_cell()} function. Time
to use the cell to create a RNN. - Build the RNN using the
\href{https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn}{\texttt{tf.nn.dynamic\_rnn()}}
- Apply the name ``final\_state'' to the final state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the outputs and final\_state state in the following tuple
\texttt{(Outputs,\ FinalState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{build\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create a RNN using a RNN Cell}
        \PY{l+s+sd}{    :param cell: RNN Cell}
        \PY{l+s+sd}{    :param inputs: Input text data}
        \PY{l+s+sd}{    :return: Tuple (Outputs, Final State)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{outputs}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{dynamic\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
            \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{final\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{outputs}\PY{p}{,} \PY{n}{final\PYZus{}state}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}rnn}\PY{p}{(}\PY{n}{build\PYZus{}rnn}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{build-the-neural-network}{%
\subsubsection{Build the Neural
Network}\label{build-the-neural-network}}

Apply the functions you implemented above to: - Apply embedding to
\texttt{input\_data} using your
\texttt{get\_embed(input\_data,\ vocab\_size,\ embed\_dim)} function. -
Build RNN using \texttt{cell} and your
\texttt{build\_rnn(cell,\ inputs)} function. - Apply a fully connected
layer with a linear activation and \texttt{vocab\_size} as the number of
outputs.

Return the logits and final state in the following tuple (Logits,
FinalState)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{def} \PY{n+nf}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Build part of the neural network}
        \PY{l+s+sd}{    :param cell: RNN cell}
        \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of rnns}
        \PY{l+s+sd}{    :param input\PYZus{}data: Input data}
        \PY{l+s+sd}{    :param vocab\PYZus{}size: Vocabulary size}
        \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
        \PY{l+s+sd}{    :return: Tuple (Logits, FinalState)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{n}{embedding\PYZus{}layer} \PY{o}{=} \PY{n}{get\PYZus{}embed}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}
            \PY{n}{rnn\PYZus{}outputs}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{build\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{embedding\PYZus{}layer}\PY{p}{)}
            \PY{n}{logits} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{contrib}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{fully\PYZus{}connected}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{n}{rnn\PYZus{}outputs}\PY{p}{,} \PY{n}{num\PYZus{}outputs}\PY{o}{=}\PY{n}{vocab\PYZus{}size}\PY{p}{,} 
                                                       \PY{n}{activation\PYZus{}fn}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{logits}\PY{p}{,} \PY{n}{final\PYZus{}state}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}nn}\PY{p}{(}\PY{n}{build\PYZus{}nn}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{batches}{%
\subsubsection{Batches}\label{batches}}

Implement \texttt{get\_batches} to create batches of input and targets
using \texttt{int\_text}. The batches should be a Numpy array with the
shape
\texttt{(number\ of\ batches,\ 2,\ batch\ size,\ sequence\ length)}.
Each batch contains two elements: - The first element is a single batch
of \textbf{input} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}} - The second element is a
single batch of \textbf{targets} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}}

If you can't fill the last batch with enough data, drop the last batch.

For example,
\texttt{get\_batches({[}1,\ 2,\ 3,\ 4,\ 5,\ 6,\ 7,\ 8,\ 9,\ 10,\ 11,\ 12,\ 13,\ 14,\ 15,\ 16,\ 17,\ 18,\ 19,\ 20{]},\ 3,\ 2)}
would return a Numpy array of the following:

\begin{verbatim}
[
  # First Batch
  [
    # Batch of Input
    [[ 1  2], [ 7  8], [13 14]]
    # Batch of targets
    [[ 2  3], [ 8  9], [14 15]]
  ]

  # Second Batch
  [
    # Batch of Input
    [[ 3  4], [ 9 10], [15 16]]
    # Batch of targets
    [[ 4  5], [10 11], [16 17]]
  ]

  # Third Batch
  [
    # Batch of Input
    [[ 5  6], [11 12], [17 18]]
    # Batch of targets
    [[ 6  7], [12 13], [18  1]]
  ]
]
\end{verbatim}

Notice that the last target value in the last batch is the first input
value of the first batch. In this case, \texttt{1}. This is a common
technique used when creating sequence batches, although it is rather
unintuitive.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Return batches of input and target}
        \PY{l+s+sd}{    :param int\PYZus{}text: Text with the words replaced by their ids}
        \PY{l+s+sd}{    :param batch\PYZus{}size: The size of batch}
        \PY{l+s+sd}{    :param seq\PYZus{}length: The length of sequence}
        \PY{l+s+sd}{    :return: Batches as a Numpy array}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            
            \PY{c+c1}{\PYZsh{} Keep only enough words in int\PYZus{}text to make full batches.}
            \PY{n}{words\PYZus{}per\PYZus{}batch} \PY{o}{=} \PY{n}{batch\PYZus{}size} \PY{o}{*} \PY{n}{seq\PYZus{}length}
            \PY{n}{number\PYZus{}of\PYZus{}batches} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{n}{words\PYZus{}per\PYZus{}batch}
            \PY{n}{words\PYZus{}to\PYZus{}keep} \PY{o}{=} \PY{n}{words\PYZus{}per\PYZus{}batch} \PY{o}{*} \PY{n}{number\PYZus{}of\PYZus{}batches}
            \PY{n}{int\PYZus{}text} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{[}\PY{p}{:}\PY{n}{words\PYZus{}to\PYZus{}keep}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Prepare for creation of the target values: }
            \PY{c+c1}{\PYZsh{} Target value of an element at a given position in int\PYZus{}text }
            \PY{c+c1}{\PYZsh{} is that of the element residing at the subsequent position.}
            \PY{n}{target\PYZus{}int\PYZus{}text} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Move the first element of the input text to the final position.}
            \PY{c+c1}{\PYZsh{} This handles the edge case of how to choose a target value for }
            \PY{c+c1}{\PYZsh{} the final element of int\PYZus{}text.}
            \PY{n}{target\PYZus{}int\PYZus{}text}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{target\PYZus{}int\PYZus{}text}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                
            \PY{c+c1}{\PYZsh{} Reshape int\PYZus{}text to have the same number of rows }
            \PY{c+c1}{\PYZsh{} as the size of the batches.}
            \PY{n}{int\PYZus{}text} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{} Also reshape target\PYZus{}int\PYZus{}text to have the same number of rows }
            \PY{c+c1}{\PYZsh{} as the size of the batches.}
            \PY{n}{target\PYZus{}int\PYZus{}text} \PY{o}{=} \PY{n}{target\PYZus{}int\PYZus{}text}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} An array to contain all the batches}
            \PY{n}{all\PYZus{}batches} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                
            \PY{c+c1}{\PYZsh{} Slice features (x) and targets (y) columns from }
            \PY{c+c1}{\PYZsh{} their respective arrays (int\PYZus{}text, target\PYZus{}int\PYZus{}text) }
            \PY{c+c1}{\PYZsh{} in order to populate the batches. Each batch contains }
            \PY{c+c1}{\PYZsh{} one sequence of values from each row of the arrays, }
            \PY{c+c1}{\PYZsh{} for both features and targets.}
            \PY{c+c1}{\PYZsh{} The start and stop of each sequence (or the indices of }
            \PY{c+c1}{\PYZsh{} of the columns that are sliced) correspond to the }
            \PY{c+c1}{\PYZsh{} seq\PYZus{}length parameter\PYZsq{}s value. }
            \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{int\PYZus{}text}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}\PY{p}{:}
                \PY{n}{x\PYZus{}batch} \PY{o}{=} \PY{n}{int\PYZus{}text}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{n}\PY{p}{:}\PY{n}{n}\PY{o}{+}\PY{n}{seq\PYZus{}length}\PY{p}{]}
                \PY{n}{y\PYZus{}batch} \PY{o}{=} \PY{n}{target\PYZus{}int\PYZus{}text}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{n}\PY{p}{:}\PY{n}{n}\PY{o}{+}\PY{n}{seq\PYZus{}length}\PY{p}{]}
                \PY{n}{all\PYZus{}batches}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{x\PYZus{}batch}\PY{p}{,} \PY{n}{y\PYZus{}batch}\PY{p}{]}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{} Make sure an np.array is returned}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{all\PYZus{}batches}\PY{p}{)}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}batches}\PY{p}{(}\PY{n}{get\PYZus{}batches}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{neural-network-training}{%
\subsection{Neural Network Training}\label{neural-network-training}}

\hypertarget{hyperparameters}{%
\subsubsection{Hyperparameters}\label{hyperparameters}}

Tune the following parameters:

\begin{itemize}
\tightlist
\item
  Set \texttt{num\_epochs} to the number of epochs.
\item
  Set \texttt{batch\_size} to the batch size.
\item
  Set \texttt{rnn\_size} to the size of the RNNs.
\item
  Set \texttt{embed\_dim} to the size of the embedding.
\item
  Set \texttt{seq\_length} to the length of sequence.
\item
  Set \texttt{learning\_rate} to the learning rate.
\item
  Set \texttt{show\_every\_n\_batches} to the number of batches the
  neural network should print progress.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{c+c1}{\PYZsh{} Number of Epochs}
         \PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{1000}
         \PY{c+c1}{\PYZsh{} Batch Size}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1000}
         \PY{c+c1}{\PYZsh{} RNN Size}
         \PY{n}{rnn\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{512}
         \PY{c+c1}{\PYZsh{} Embedding Dimension Size}
         \PY{n}{embed\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{300}
         \PY{c+c1}{\PYZsh{} Sequence Length}
         \PY{n}{seq\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{c+c1}{\PYZsh{} Learning Rate}
         \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.001}
         \PY{c+c1}{\PYZsh{} Show stats for every n number of batches}
         \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{=} \PY{l+m+mi}{60}
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{save\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./save}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \hypertarget{build-the-graph}{%
\subsubsection{Build the Graph}\label{build-the-graph}}

Build the graph using the neural network you implemented.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{contrib} \PY{k}{import} \PY{n}{seq2seq}
         
         \PY{n}{train\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
         \PY{k}{with} \PY{n}{train\PYZus{}graph}\PY{o}{.}\PY{n}{as\PYZus{}default}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{vocab\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
             \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{n}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}
             \PY{n}{input\PYZus{}data\PYZus{}shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{input\PYZus{}text}\PY{p}{)}
             \PY{n}{cell}\PY{p}{,} \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}
             \PY{n}{logits}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Probabilities for generating words}
             \PY{n}{probs} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{logits}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{probs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Loss function}
             \PY{n}{cost} \PY{o}{=} \PY{n}{seq2seq}\PY{o}{.}\PY{n}{sequence\PYZus{}loss}\PY{p}{(}
                 \PY{n}{logits}\PY{p}{,}
                 \PY{n}{targets}\PY{p}{,}
                 \PY{n}{tf}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Optimizer}
             \PY{n}{optimizer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{AdamOptimizer}\PY{p}{(}\PY{n}{lr}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Gradient Clipping}
             \PY{n}{gradients} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{compute\PYZus{}gradients}\PY{p}{(}\PY{n}{cost}\PY{p}{)}
             \PY{n}{capped\PYZus{}gradients} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{clip\PYZus{}by\PYZus{}value}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} \PY{n}{var}\PY{p}{)} \PY{k}{for} \PY{n}{grad}\PY{p}{,} \PY{n}{var} \PY{o+ow}{in} \PY{n}{gradients} \PY{k}{if} \PY{n}{grad} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}
             \PY{n}{train\PYZus{}op} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{apply\PYZus{}gradients}\PY{p}{(}\PY{n}{capped\PYZus{}gradients}\PY{p}{)}
\end{Verbatim}


    \hypertarget{train}{%
\subsection{Train}\label{train}}

Train the neural network on the preprocessed data. If you have a hard
time getting a good loss, check the
\href{https://discussions.udacity.com/}{forums} to see if anyone is
having the same problem.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{batches} \PY{o}{=} \PY{n}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}
         
         \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{train\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
             \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{global\PYZus{}variables\PYZus{}initializer}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{for} \PY{n}{epoch\PYZus{}i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n}{state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{batches}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
         
                 \PY{k}{for} \PY{n}{batch\PYZus{}i}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{:}
                     \PY{n}{feed} \PY{o}{=} \PY{p}{\PYZob{}}
                         \PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{x}\PY{p}{,}
                         \PY{n}{targets}\PY{p}{:} \PY{n}{y}\PY{p}{,}
                         \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{state}\PY{p}{,}
                         \PY{n}{lr}\PY{p}{:} \PY{n}{learning\PYZus{}rate}\PY{p}{\PYZcb{}}
                     \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{p}{[}\PY{n}{cost}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{train\PYZus{}op}\PY{p}{]}\PY{p}{,} \PY{n}{feed}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} Show every \PYZlt{}show\PYZus{}every\PYZus{}n\PYZus{}batches\PYZgt{} batches}
                     \PY{k}{if} \PY{p}{(}\PY{p}{(}\PY{n}{epoch\PYZus{}i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)} \PY{o}{+} \PY{n}{batch\PYZus{}i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}:\PYZgt{}3\PYZcb{}}\PY{l+s+s1}{ Batch }\PY{l+s+si}{\PYZob{}:\PYZgt{}4\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{   train\PYZus{}loss = }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                             \PY{n}{epoch\PYZus{}i}\PY{p}{,}
                             \PY{n}{batch\PYZus{}i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}
                             \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{,}
                             \PY{n}{train\PYZus{}loss}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Save Model}
             \PY{n}{saver} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{Saver}\PY{p}{(}\PY{p}{)}
             \PY{n}{saver}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Trained and Saved}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch  10 Batch    6/6   train\_loss = 5.506
Epoch  20 Batch    6/6   train\_loss = 4.901
Epoch  30 Batch    6/6   train\_loss = 4.466
Epoch  40 Batch    6/6   train\_loss = 4.127
Epoch  50 Batch    6/6   train\_loss = 3.835
Epoch  60 Batch    6/6   train\_loss = 3.574
Epoch  70 Batch    6/6   train\_loss = 3.344
Epoch  80 Batch    6/6   train\_loss = 3.125
Epoch  90 Batch    6/6   train\_loss = 2.937
Epoch 100 Batch    6/6   train\_loss = 2.755
Epoch 110 Batch    6/6   train\_loss = 2.597
Epoch 120 Batch    6/6   train\_loss = 2.465
Epoch 130 Batch    6/6   train\_loss = 2.310
Epoch 140 Batch    6/6   train\_loss = 2.198
Epoch 150 Batch    6/6   train\_loss = 2.082
Epoch 160 Batch    6/6   train\_loss = 1.978
Epoch 170 Batch    6/6   train\_loss = 1.855
Epoch 180 Batch    6/6   train\_loss = 1.780
Epoch 190 Batch    6/6   train\_loss = 1.657
Epoch 200 Batch    6/6   train\_loss = 1.582
Epoch 210 Batch    6/6   train\_loss = 1.505
Epoch 220 Batch    6/6   train\_loss = 1.442
Epoch 230 Batch    6/6   train\_loss = 1.382
Epoch 240 Batch    6/6   train\_loss = 1.283
Epoch 250 Batch    6/6   train\_loss = 1.231
Epoch 260 Batch    6/6   train\_loss = 1.167
Epoch 270 Batch    6/6   train\_loss = 1.121
Epoch 280 Batch    6/6   train\_loss = 1.069
Epoch 290 Batch    6/6   train\_loss = 1.013
Epoch 300 Batch    6/6   train\_loss = 0.965
Epoch 310 Batch    6/6   train\_loss = 0.925
Epoch 320 Batch    6/6   train\_loss = 0.893
Epoch 330 Batch    6/6   train\_loss = 0.869
Epoch 340 Batch    6/6   train\_loss = 0.808
Epoch 350 Batch    6/6   train\_loss = 0.787
Epoch 360 Batch    6/6   train\_loss = 0.765
Epoch 370 Batch    6/6   train\_loss = 0.741
Epoch 380 Batch    6/6   train\_loss = 0.710
Epoch 390 Batch    6/6   train\_loss = 0.684
Epoch 400 Batch    6/6   train\_loss = 0.668
Epoch 410 Batch    6/6   train\_loss = 0.654
Epoch 420 Batch    6/6   train\_loss = 0.649
Epoch 430 Batch    6/6   train\_loss = 0.623
Epoch 440 Batch    6/6   train\_loss = 0.595
Epoch 450 Batch    6/6   train\_loss = 0.591
Epoch 460 Batch    6/6   train\_loss = 0.579
Epoch 470 Batch    6/6   train\_loss = 0.563
Epoch 480 Batch    6/6   train\_loss = 0.561
Epoch 490 Batch    6/6   train\_loss = 0.554
Epoch 500 Batch    6/6   train\_loss = 0.545
Epoch 510 Batch    6/6   train\_loss = 0.529
Epoch 520 Batch    6/6   train\_loss = 0.525
Epoch 530 Batch    6/6   train\_loss = 0.512
Epoch 540 Batch    6/6   train\_loss = 0.505
Epoch 550 Batch    6/6   train\_loss = 0.508
Epoch 560 Batch    6/6   train\_loss = 0.503
Epoch 570 Batch    6/6   train\_loss = 0.493
Epoch 580 Batch    6/6   train\_loss = 0.491
Epoch 590 Batch    6/6   train\_loss = 0.483
Epoch 600 Batch    6/6   train\_loss = 0.481
Epoch 610 Batch    6/6   train\_loss = 0.467
Epoch 620 Batch    6/6   train\_loss = 0.467
Epoch 630 Batch    6/6   train\_loss = 0.465
Epoch 640 Batch    6/6   train\_loss = 0.466
Epoch 650 Batch    6/6   train\_loss = 0.456
Epoch 660 Batch    6/6   train\_loss = 0.460
Epoch 670 Batch    6/6   train\_loss = 0.451
Epoch 680 Batch    6/6   train\_loss = 0.444
Epoch 690 Batch    6/6   train\_loss = 0.444
Epoch 700 Batch    6/6   train\_loss = 0.444
Epoch 710 Batch    6/6   train\_loss = 0.432
Epoch 720 Batch    6/6   train\_loss = 0.442
Epoch 730 Batch    6/6   train\_loss = 0.432
Epoch 740 Batch    6/6   train\_loss = 0.444
Epoch 750 Batch    6/6   train\_loss = 0.436
Epoch 760 Batch    6/6   train\_loss = 0.433
Epoch 770 Batch    6/6   train\_loss = 0.426
Epoch 780 Batch    6/6   train\_loss = 0.426
Epoch 790 Batch    6/6   train\_loss = 0.418
Epoch 800 Batch    6/6   train\_loss = 0.423
Epoch 810 Batch    6/6   train\_loss = 0.424
Epoch 820 Batch    6/6   train\_loss = 0.422
Epoch 830 Batch    6/6   train\_loss = 0.417
Epoch 840 Batch    6/6   train\_loss = 0.417
Epoch 850 Batch    6/6   train\_loss = 0.413
Epoch 860 Batch    6/6   train\_loss = 0.414
Epoch 870 Batch    6/6   train\_loss = 0.416
Epoch 880 Batch    6/6   train\_loss = 0.405
Epoch 890 Batch    6/6   train\_loss = 0.409
Epoch 900 Batch    6/6   train\_loss = 0.410
Epoch 910 Batch    6/6   train\_loss = 0.407
Epoch 920 Batch    6/6   train\_loss = 0.402
Epoch 930 Batch    6/6   train\_loss = 0.403
Epoch 940 Batch    6/6   train\_loss = 0.402
Epoch 950 Batch    6/6   train\_loss = 0.399
Epoch 960 Batch    6/6   train\_loss = 0.404
Epoch 970 Batch    6/6   train\_loss = 0.396
Epoch 980 Batch    6/6   train\_loss = 0.399
Epoch 990 Batch    6/6   train\_loss = 0.396
Epoch 1000 Batch    6/6   train\_loss = 0.391
Model Trained and Saved

    \end{Verbatim}

    \hypertarget{save-parameters}{%
\subsection{Save Parameters}\label{save-parameters}}

Save \texttt{seq\_length} and \texttt{save\_dir} for generating a new TV
script.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{c+c1}{\PYZsh{} Save parameters for checkpoint}
         \PY{n}{helper}\PY{o}{.}\PY{n}{save\PYZus{}params}\PY{p}{(}\PY{p}{(}\PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \hypertarget{checkpoint}{%
\section{Checkpoint}\label{checkpoint}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{helper}
         \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
         
         \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
         \PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{load\PYZus{}dir} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \hypertarget{implement-generate-functions}{%
\subsection{Implement Generate
Functions}\label{implement-generate-functions}}

\hypertarget{get-tensors}{%
\subsubsection{Get Tensors}\label{get-tensors}}

Get tensors from \texttt{loaded\_graph} using the function
\href{https://www.tensorflow.org/api_docs/python/tf/Graph\#get_tensor_by_name}{\texttt{get\_tensor\_by\_name()}}.
Get the tensors using the following names: - ``input:0'' -
``initial\_state:0'' - ``final\_state:0'' - ``probs:0''

Return the tensors in the following tuple
\texttt{(InputTensor,\ InitialStateTensor,\ FinalStateTensor,\ ProbsTensor)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Get input, initial state, final state, and probabilities tensor from \PYZlt{}loaded\PYZus{}graph\PYZgt{}}
         \PY{l+s+sd}{    :param loaded\PYZus{}graph: TensorFlow graph loaded from file}
         \PY{l+s+sd}{    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             \PY{n}{input\PYZus{}tensor} \PY{o}{=} \PY{n}{loaded\PYZus{}graph}\PY{o}{.}\PY{n}{get\PYZus{}tensor\PYZus{}by\PYZus{}name}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{input:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{initial\PYZus{}state\PYZus{}tensor} \PY{o}{=} \PY{n}{loaded\PYZus{}graph}\PY{o}{.}\PY{n}{get\PYZus{}tensor\PYZus{}by\PYZus{}name}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{initial\PYZus{}state:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{final\PYZus{}state\PYZus{}tensor} \PY{o}{=} \PY{n}{loaded\PYZus{}graph}\PY{o}{.}\PY{n}{get\PYZus{}tensor\PYZus{}by\PYZus{}name}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{final\PYZus{}state:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{probs\PYZus{}tensor} \PY{o}{=} \PY{n}{loaded\PYZus{}graph}\PY{o}{.}\PY{n}{get\PYZus{}tensor\PYZus{}by\PYZus{}name}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{probs:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{return} \PY{n}{input\PYZus{}tensor}\PY{p}{,} \PY{n}{initial\PYZus{}state\PYZus{}tensor}\PY{p}{,} \PY{n}{final\PYZus{}state\PYZus{}tensor}\PY{p}{,} \PY{n}{probs\PYZus{}tensor}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}tensors}\PY{p}{(}\PY{n}{get\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{choose-word}{%
\subsubsection{Choose Word}\label{choose-word}}

Implement the \texttt{pick\_word()} function to select the next word
using \texttt{probabilities}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{k}{def} \PY{n+nf}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Pick the next word in the generated text}
         \PY{l+s+sd}{    :param probabilities: Probabilites of the next word}
         \PY{l+s+sd}{    :param int\PYZus{}to\PYZus{}vocab: Dictionary of word ids as the keys and words as the values}
         \PY{l+s+sd}{    :return: String of the predicted word}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} TODO: Implement Function}
             
             \PY{c+c1}{\PYZsh{} Randomly choose a word from the four words that have }
             \PY{c+c1}{\PYZsh{} the highest probabilities.}
             \PY{n}{top\PYZus{}n} \PY{o}{=} \PY{l+m+mi}{4}
             
             \PY{c+c1}{\PYZsh{} Get the indices corresponding to the top probilities}
             \PY{n}{idx\PYZus{}top\PYZus{}probs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argpartition}\PY{p}{(}\PY{n}{probabilities}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{top\PYZus{}n}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{top\PYZus{}n}\PY{p}{:}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} Choose one of these indices at random}
             \PY{n}{choice} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{idx\PYZus{}top\PYZus{}probs}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Select the word that belongs to the randomly }
             \PY{c+c1}{\PYZsh{} chosen index.}
             \PY{n}{next\PYZus{}word} \PY{o}{=} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{[}\PY{n}{choice}\PY{p}{]}
             
             \PY{k}{return} \PY{n}{next\PYZus{}word}
         
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}pick\PYZus{}word}\PY{p}{(}\PY{n}{pick\PYZus{}word}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests Passed

    \end{Verbatim}

    \hypertarget{generate-tv-script}{%
\subsection{Generate TV Script}\label{generate-tv-script}}

This will generate the TV script for you. Set \texttt{gen\_length} to
the length of TV script you want to generate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{gen\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{500}
         \PY{c+c1}{\PYZsh{} homer\PYZus{}simpson, moe\PYZus{}szyslak, or Barney\PYZus{}Gumble}
         \PY{n}{prime\PYZus{}word} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{moe\PYZus{}szyslak}\PY{l+s+s1}{\PYZsq{}}
         
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
         \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{n}{loaded\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
         \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{loaded\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Load saved model}
             \PY{n}{loader} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{import\PYZus{}meta\PYZus{}graph}\PY{p}{(}\PY{n}{load\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.meta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{loader}\PY{o}{.}\PY{n}{restore}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{load\PYZus{}dir}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Get Tensors from loaded model}
             \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{probs} \PY{o}{=} \PY{n}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Sentences generation setup}
             \PY{n}{gen\PYZus{}sentences} \PY{o}{=} \PY{p}{[}\PY{n}{prime\PYZus{}word} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Generate sentences}
             \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{gen\PYZus{}length}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Dynamic Input}
                 \PY{n}{dyn\PYZus{}input} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{gen\PYZus{}sentences}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{seq\PYZus{}length}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{]}
                 \PY{n}{dyn\PYZus{}seq\PYZus{}length} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dyn\PYZus{}input}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} Get Prediction}
                 \PY{n}{probabilities}\PY{p}{,} \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}
                     \PY{p}{[}\PY{n}{probs}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{]}\PY{p}{,}
                     \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{dyn\PYZus{}input}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{prev\PYZus{}state}\PY{p}{\PYZcb{}}\PY{p}{)}
                 
                 \PY{n}{pred\PYZus{}word} \PY{o}{=} \PY{n}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{dyn\PYZus{}seq\PYZus{}length}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
         
                 \PY{n}{gen\PYZus{}sentences}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pred\PYZus{}word}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Remove tokens}
             \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{gen\PYZus{}sentences}\PY{p}{)}
             \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{token} \PY{o+ow}{in} \PY{n}{token\PYZus{}dict}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{ending} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
                 \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{token}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{p}{)}
             \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{( }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{tv\PYZus{}script}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:Restoring parameters from ./save
moe\_szyslak: yeah on the ball thing.
lenny\_leonard: i see this for now since there who makes sense home she. while.
carl\_carlson: pick up! what?
moe\_szyslak: just what have money for the president of course that old friend, boxing i was! another moe
barney\_gumble: hey! what, you're the fat one else stupid(to, lenny homer? hey) they the should put on how a daddy? 'cause you sayin' you look, but that you've had all a feeling moe\_szyslak: him for / yeah with the"
ned\_flanders: hey guys stink, usin'. it's husband at
homer\_simpson:(realizing at a man sorry homer then) there, didn't be" eightball" problem?!
homer\_simpson: thanks for homer?. uh, would.
apu\_nahasapeemapetilon: i could this guy for your old man passed, two wanted ya got in good?
barney\_gumble:(a beer to the real"-- then tonight, then singing as moe's.. some with dirt.(counting sign at the bar)
, ah, who homer\_simpson: on the phone and drink where? i! i own losers all better again
barney\_gumble: that's alright what does she got about the money
homer\_simpson: and i are you! 'cause i will say no trouble. could pull it off my face! ow? my poor man!
david\_byrne: wait right the party at this bar is(a beer) what are you?, didn't? the one, hey(determined mean a day that's drive a body? where you could do up because they enveloped me with marge 'cause" joe could" save no".
moe\_szyslak: the super are my best friend is my one? you've have to be second, booze like(with each woman).(art). i could really do it, you! am(raises prank watch) better were outta the the man. the only" no girl, of any? hey that's what its fondest me last with alcohol again!. whee have in there you open up? but. not to their choice. and it's

lisa\_simpson: a duff!
barney\_gumble: you like for be sorry, homer the clone). you know, no was from here?.. it, uh., i wanna have a job for well that you still in" here no" i wanna need? another.
seymour\_skinner:(sings, disgusted lloyd to us sits there around, stick tonight. i'll have. to uh.(takes out cell phone
bart\_simpson: hi while did yourself is here puff in the car!
moe\_szyslak: i mean. now i let,

    \end{Verbatim}

    \hypertarget{the-tv-script-is-nonsensical}{%
\section{The TV Script is
Nonsensical}\label{the-tv-script-is-nonsensical}}

It's ok if the TV script doesn't make any sense. We trained on less than
a megabyte of text. In order to get good results, you'll have to use a
smaller vocabulary or get more data. Luckily there's more data! As we
mentioned in the beggining of this project, this is a subset of
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{another
dataset}. We didn't have you train on all the data, because that would
take too long. However, you are free to train your neural network on all
the data. After you complete the project, of course. \# Submitting This
Project When submitting this project, make sure to run all the cells
before saving the notebook. Save the notebook file as
``dlnd\_tv\_script\_generation.ipynb'' and save it as a HTML file under
``File'' -\textgreater{} ``Download as''. Include the ``helper.py'' and
``problem\_unittests.py'' files in your submission.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
